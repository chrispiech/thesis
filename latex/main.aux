\relax 
\providecommand\zref@newlabel[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{Preface}{iv}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{v}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Shortage of Quality Education}{1}}
\citation{annualOECDReport}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Increase Productivity}{2}}
\citation{sadler2006impact}
\citation{henke1996schools}
\citation{henke1996schools}
\citation{henke1996schools}
\citation{corbett2001cognitive}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Feedback}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Teacher time}}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:dataTable}{{1.1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Online Programming Case Study}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The Problem is Hard}{5}}
\citation{piech2012modeling}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Emergent Structure}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Unique Access to Data}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  The different types of assignments covered in this thesis. \relax }}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Summary of datasets}}{7}}
\newlabel{tab:dataTable}{{1.2}{7}}
\citation{piech2012modeling}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Approaches}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Novel Contribution}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}A Vision for the Future}{8}}
\newlabel{fig:introProjects}{{1.2(a)}{9}}
\newlabel{sub@fig:introProjects}{{(a)}{9}}
\newlabel{fig:introImpact}{{1.2(b)}{9}}
\newlabel{sub@fig:introImpact}{{(b)}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  \ref  {sub@fig:introProjects} What I did. \ref  {sub@fig:introImpact} How it fits in with previous research. \relax }}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{9}}
\citation{polson2013foundations}
\citation{corbett2001cognitive}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep Knowledge Tracing}{10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Knowledge Tracing Task}{10}}
\citation{corbett1994knowledge}
\citation{linnenbrink2004role}
\citation{elliot2013handbook}
\citation{cohen2008identity}
\citation{fitch2005evolution}
\citation{gentner1983structure}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A single student and her predicted responses as she solves 50 exercises on Khan Academy 8th grade math curriculum. She seems to master finding x and y intercepts and then has trouble transferring knowledge to graphing linear equations.  \relax }}{11}}
\newlabel{fig:singleStudent}{{2.1}{11}}
\citation{corbett1994knowledge}
\citation{d2008more}
\citation{yudelson2013individualized}
\citation{pardos2011kt}
\citation{schraagen2000cognitive}
\citation{rafferty2011faster}
\citation{pavlik2009performance}
\citation{cen2006learning}
\citation{gong2010comparing}
\citation{d2011ensembling}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Related Work}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Bayesian Knowledge Tracing}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Other Dynamic Probabilistic Models}{12}}
\citation{lan2014time}
\citation{khajah2014integrating}
\citation{khajah2014incorporating}
\citation{williams1989learning}
\citation{hochreiter1997long}
\citation{graves2013speech}
\citation{mikolov2010recurrent}
\citation{karpathy2014deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Recurrent Neural Networks}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Knowledge Tracing}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Model}{13}}
\citation{hochreiter1997long}
\citation{baraniuk2007compressive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Input and Output Time Series}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The connection between variables in a simple recurrent neural network. The inputs ($\mathbf  x_t$) to the dynamic network are either one-hot encodings or compressed representations of a student action, and the prediction ($\mathbf  y_t$) is a vector representing the probability of getting each of the dataset exercises correct.  \relax }}{15}}
\newlabel{fig:rnn}{{2.2}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Optimization}{15}}
\citation{rohrer2009effects}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Educational Applications}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Improving Curricula}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Discovering Exercise Relationships}{16}}
\newlabel{eq influence}{{2.4}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Datasets}{16}}
\citation{drasgow1990item}
\citation{KAprivacy}
\citation{feng2009addressing}
\citation{pardos2011kt}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Results}{17}}
\newlabel{sec results}{{2.6}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Knowledge tracing accuracy}}{18}}
\newlabel{table:results}{{2.1}{18}}
\newlabel{fig:toyAcc}{{2.3(a)}{18}}
\newlabel{sub@fig:toyAcc}{{(a)}{18}}
\newlabel{fig:khanRoc}{{2.3(b)}{18}}
\newlabel{sub@fig:khanRoc}{{(b)}{18}}
\newlabel{fig:curriculum}{{2.3(c)}{18}}
\newlabel{sub@fig:curriculum}{{(c)}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Left: Prediction results for (a) simulated data and (b) Khan Academy data. Right: (c) Predicted knowledge on Assistments data for different exercise curricula. Error bars are standard error of the mean.\relax }}{18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}$\mathrm  {Expectimax}$ Curricula}{18}}
\newlabel{sec expectimax}{{2.6.1}{18}}
\citation{piech2015autonomously}
\citation{piech2012modeling}
\citation{piechICML15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Discovered Exercise Relationships}{19}}
\newlabel{sec ex rel}{{2.6.2}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Discussion}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Graphs of conditional influence between exercises in DKT models. Above: We observe a perfect clustering of latent concepts in the synthetic data. Below: A convincing depiction of how 8th grade math Common Core exercises influence one another. Arrow size indicates connection strength. Note that nodes may be connected in both directions. Edges with a magnitude smaller than 0.1 have been thresholded. Cluster labels are added by hand, but are fully consistent with the exercises in each cluster.  \relax }}{20}}
\newlabel{fig:conceptClusters}{{2.4}{20}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Problem Solving Policies}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{pappano2012year}
\citation{rivers2014automating}
\citation{barnes2008toward}
\citation{singh2013automated}
\citation{cheang2003automated}
\newlabel{pspDef}{{1}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Related Work}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A visualization of a problem solving policy generated for an early Hour of Code challenge. Each node is a unique partial solution, node 0 is the correct answer. The edges show what next partial solution we think an expert would suggest students move towards.\relax }}{24}}
\newlabel{fig:policy}{{3.1}{24}}
\citation{weld2012personalized}
\citation{watson2012bluefix}
\citation{nguyen2014codewebs}
\citation{rivers2012canonicalizing}
\citation{kulkarni2013peer}
\citation{ritter2007cognitive}
\citation{aleven2002effective}
\citation{fitzgerald2008debugging}
\citation{ahmadzadeh2005analysis}
\citation{puterman2009markov}
\citation{szczerba2000robust}
\citation{whitehill2009whose}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Schematic of the maze and example solution for P\textsubscript  {A}\hspace  {0.5mm}(left) and P\textsubscript  {B}\hspace  {0.5mm}(right). The arrow is the agent and the heart is the goal.\relax }}{26}}
\newlabel{fig:hocExample}{{3.2}{26}}
\newlabel{fig:retention}{{3.3(a)}{26}}
\newlabel{sub@fig:retention}{{(a)}{26}}
\newlabel{fig:coverage}{{3.3(b)}{26}}
\newlabel{sub@fig:coverage}{{(b)}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  \ref  {sub@fig:retention} Percent of students who finished the first problem in the hour of code that solve the remaining ones. \ref  {sub@fig:coverage} The percent of submissions that are covered by annotating the most popular partial solutions. \relax }}{26}}
\newlabel{tab:predacc}{{3.3}{26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{26}}
\citation{fleiss1981measurement}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of the dataset analyzed.\relax }}{28}}
\newlabel{tab:dataTable}{{3.1}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Methods}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Desirable Path Algorithms}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces How does node popularity relate to number of students who agree with experts?\relax }}{29}}
\newlabel{fig:confidence}{{3.4}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Poisson Path}{29}}
\newlabel{pcpDef}{{2}{30}}
\@writefile{toc}{\contentsline {subsubsection}{Independent Probable Path}{30}}
\citation{shapley1953stochastic}
\citation{rivers2014automating}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Baseline Algorithms}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Markov Decision Problem}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Rivers Policy}{31}}
\citation{whitehill2009whose}
\@writefile{toc}{\contentsline {subsubsection}{Static Analysis}{32}}
\@writefile{toc}{\contentsline {subsubsection}{Most Likely Next}{32}}
\@writefile{toc}{\contentsline {subsubsection}{Most Common Path}{32}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Success}{32}}
\@writefile{toc}{\contentsline {subsubsection}{Min Time}{32}}
\@writefile{toc}{\contentsline {subsubsection}{Ability Model}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Evaluation}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Results}{33}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Percent submissions with the ground truth edge correctly predicted. $^\dagger $ Algorithms applied to learning PSP's in other papers. $^\ddagger $ Desirable Path Algorithms.\relax }}{34}}
\newlabel{tab:results1}{{3.2}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Learning curve for P\textsubscript  {A}\hspace  {0.5mm}(above) and P\textsubscript  {B}\hspace  {0.5mm}(bellow). Students are subsampled randomly from dataset. Error bars are standard error.\relax }}{35}}
\newlabel{fig:learningCurve}{{3.5}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Summative Assessment}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Student path scores vs probability of completion of subsequent problems. Error bars are standard error. \relax }}{36}}
\newlabel{fig:predacc}{{3.6}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Generating Hints}{37}}
\citation{huang2013syntactic}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Discussion}{38}}
\citation{roll2014benefits}
\citation{piech2012modeling}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Conclusion}{39}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Homework Embedding Spaces}{41}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{huangetal13}
\citation{basu2013powergrading}
\citation{nguyen14}
\citation{brooks2014divide}
\citation{lan2015mathematical}
\citation{piech2015}
\citation{huangetal13}
\citation{rogers2014aces}
\citation{mokbel2013domain}
\citation{nguyen14}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Related Work}{42}}
\newlabel{sec:related}{{4.1}{42}}
\citation{socher2013recursive}
\citation{socher2011semi}
\citation{zaremba2014learningb}
\citation{bowman2013can}
\citation{ovsjanikov2012functional}
\citation{ovsjanikov2013analysis}
\citation{song2013kernel}
\citation{song2009hilbert}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces We learn matrices which capture functionality. Left: a student partial solution. Right: learned matrices for the syntax trees rooted at each node of placeRow.\relax }}{43}}
\newlabel{fig:splash}{{4.1}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Embedding Hoare Triples}{43}}
\newlabel{sec:embedding}{{4.2}{43}}
\citation{hoare1969axiomatic}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces  Diagram of the model for a program $A$ implementing a simple ``step forward'' behavior in a small 1-dimensional gridworld. Two of the $k$ Hoare triples that correspond with $A$ are shown. Typical worlds are larger and programs are more complex. \relax }}{44}}
\newlabel{fig:hoare}{{4.2}{44}}
\newlabel{eqn:linearmap}{{4.1}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Neural network encoding and decoding of states}{45}}
\newlabel{eqn:encodeprecondition}{{4.2}{45}}
\newlabel{eqn:decodeprecondition}{{4.3}{45}}
\newlabel{eqn:decodepostcondition}{{4.4}{45}}
\citation{bergstra2012random}
\citation{duchi2011adaptive}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Nonparametric model of program embedding}{46}}
\newlabel{sec:nonparametric}{{4.2.2}{46}}
\newlabel{eqn:objective}{{4.6}{46}}
\citation{socher2013recursive}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Triple Extraction}{47}}
\newlabel{sec:triplets}{{4.2.3}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Feedback Propagation}{47}}
\newlabel{sec:feedback}{{4.3}{47}}
\citation{goller1996learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Incorporating structure via recursive embedding}{48}}
\newlabel{eqn:feedbackRnn}{{4.7}{48}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Dataset summary. Programs are considered identical if they have equal ASTs. Unique states are different configurations of the gridworld which occur in student programs.\relax }}{49}}
\newlabel{tab:datasummary}{{4.1}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Datasets}{49}}
\newlabel{sec:data}{{4.4}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Results}{50}}
\newlabel{sec:experiments}{{4.5}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Prediction of postcondition}{50}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Test set postcondition prediction accuracy on the three programming problems. Training set results in parentheses.\relax }}{51}}
\newlabel{tab:results1}{{4.2}{51}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Evaluation of composability of embedding matrices: Accuracy on 5k random triples with ASTs rooted at $\mathbf  {block}$ nodes. NPM-0 does not jointly optimize. \relax }}{51}}
\newlabel{tab:results2}{{4.3}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Composability of program embeddings}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Prediction of Feedback}{51}}
\citation{nguyen14}
\newlabel{fig:fmHoc}{{4.3(a)}{52}}
\newlabel{sub@fig:fmHoc}{{(a)}{52}}
\newlabel{fig:fmNews}{{4.3(b)}{52}}
\newlabel{sub@fig:fmNews}{{(b)}{52}}
\newlabel{fig:fmMid}{{4.3(c)}{52}}
\newlabel{sub@fig:fmMid}{{(c)}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  Recall of feedback propagation as a function of precision for three programming problems: \ref  {sub@fig:fmHoc} $\Omega _1$, \ref  {sub@fig:fmNews} $\Omega _2$, and \ref  {sub@fig:fmMid} $\Omega _3$. On each, we compare our NPM-RNN against the RNN method and two other baselines (bag of trees and unit tests). \relax }}{52}}
\newlabel{fig:force}{{4.3}{52}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{52}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{52}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{52}}
\citation{mccabe1976complexity}
\newlabel{fig:cycloPost}{{4.4(a)}{53}}
\newlabel{sub@fig:cycloPost}{{(a)}{53}}
\newlabel{fig:cycloFm}{{4.4(b)}{53}}
\newlabel{sub@fig:cycloFm}{{(b)}{53}}
\newlabel{fig:whereWrong}{{4.4(c)}{53}}
\newlabel{sub@fig:whereWrong}{{(c)}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces  \ref  {sub@fig:cycloPost} NPM and RNN postcondition prediction accuracy as a function of cyclomatic complexity of submitted programs; \ref  {sub@fig:cycloFm} NPM-RNN and RNN feedback propagation recall (at 90\% precision). Note that the ratio of human graded assignments to number of programs is much higher in this experiment than Figure~\ref  {fig:force}; \ref  {sub@fig:whereWrong} A breakdown of the accuracy of the nonparametric model by feedback type for $\Omega _3$ (black dots). The gray bars histogram the feedback types by frequency. \relax }}{53}}
\newlabel{fig:cyclomatic}{{4.4}{53}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{53}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{53}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Code complexity and performance}{53}}
\citation{graves2014neural}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Discussion}{54}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Shared Substructures}{55}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{paul94}
\citation{thummalapenta07}
\citation{hummel08}
\citation{kim10}
\citation{piech13}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Statistics of the ML class dataset. \relax }}{58}}
\newlabel{tab:datasetsummary}{{5.1}{58}}
\newlabel{fig:codeexample}{{5.1(a)}{58}}
\newlabel{sub@fig:codeexample}{{(a)}{58}}
\newlabel{fig:astexample2}{{5.1(b)}{58}}
\newlabel{sub@fig:astexample2}{{(b)}{58}}
\newlabel{fig:subtreeexample}{{5.1(c)}{58}}
\newlabel{sub@fig:subtreeexample}{{(c)}{58}}
\newlabel{fig:contextexample}{{5.1(d)}{58}}
\newlabel{sub@fig:contextexample}{{(d)}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces  \ref  {sub@fig:codeexample} Example code submission to the ``Gradient descent (for linear regression)'' problem; \ref  {sub@fig:astexample2} Example abstract syntax tree for linear regression gradient expression: $X'*(X*\theta - y)$; \ref  {sub@fig:subtreeexample} Example subtree from the AST from Figure~\ref  {fig:astexample2}; \ref  {sub@fig:contextexample} Context of the subtree with respect to the same AST. Note the additional node denoting the ``replacement site'' of the context. \relax }}{58}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{58}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{58}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{58}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{58}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Data from a programming intensive MOOC}{58}}
\newlabel{sec:dataset}{{5.1}{58}}
\citation{eaton97}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Efficient Indexing of code submissions}{59}}
\newlabel{sec:indexing}{{5.2}{59}}
\citation{zobel06}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Efficient matching}{60}}
\citation{shasha94}
\citation{kim10}
\citation{huang13}
\citation{rivers13}
\citation{paul94}
\citation{xu03}
\citation{rivers12}
\citation{rivers13}
\citation{rivers12}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Data driven discovery of semantic equivalence classes}{61}}
\newlabel{sec:equivalence}{{5.3}{61}}
\newlabel{ex:lengthy}{{1}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Testing for semantic equivalence}{63}}
\newlabel{sec:testing}{{5.3.1}{63}}
\newlabel{def:exactequivalence}{{2}{63}}
\newlabel{def:equivalence}{{3}{63}}
\newlabel{eqn:def1}{{5.1}{63}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Pseudocode for estimating the semantic equivalence probability between two subtrees, $\mathcal  {B}$ and $\mathcal  {B}'$. The function $\texttt  {QueryIndex}(\mathcal  {B})$ is assumed to return a list of pairs $(\mathcal  {A}_i,c_i)$ such that each $\mathcal  {A}_i$ contains $\mathcal  {B}$ and $c_i$ is the number of submissions matching $\mathcal  {A}_i$. Note that the inner loop can be implemented efficiently using the precomputed hashes of global contexts (described in Section~\ref  {sec:indexing}). \relax }}{64}}
\newlabel{alg:estimate}{{1}{64}}
\newlabel{eqn:equiv2}{{5.3}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Semantic equivalence classes and solution space reductions}{64}}
\newlabel{sec:reduction}{{5.3.2}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces  Given a subtree $\mathcal  {B}$ of an AST $\mathcal  {A}$, we query the Codewebs index for other subtrees that also occur under the same context $(\mathcal  {A}\delimiter "026E30F \mathcal  {B})$. In this case, $\mathcal  {B}_1, \dots  , \mathcal  {B}_4$ would each be considered as candidates for the equivalence class of subtree $\mathcal  {B}$ if the unit test outcomes for their corresponding submissions matched that of the original AST $\mathcal  {A}$. \relax }}{65}}
\newlabel{fig:cartoon}{{5.2}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces  Example code snippets corresponding to AST subtrees tagged by an instructor for the linear regression problem. \relax }}{66}}
\newlabel{fig:tagging}{{5.3}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces  Selections of code snippets algorithmically determined for the linear regression homework. Note that not all subtrees from the equivalence classes are shown. \relax }}{67}}
\newlabel{fig:equivalences}{{5.4}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Bug finding from search queries}{67}}
\citation{eaton97}
\newlabel{fig:numastsvsruntime}{{5.5(a)}{68}}
\newlabel{sub@fig:numastsvsruntime}{{(a)}{68}}
\newlabel{fig:astsizevsruntime}{{5.5(b)}{68}}
\newlabel{sub@fig:astsizevsruntime}{{(b)}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces \ref  {sub@fig:numastsvsruntime} Runtime in seconds for indexing a collection of ASTs (as a function of the number of ASTs) from the ``gradient descent (for linear regression)'' problem; \ref  {sub@fig:astsizevsruntime} Runtime in seconds for indexing 1000 ASTs from each of the homework problems for Coursera's ML course plotted against average AST size (\# nodes) for each problem\relax }}{68}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{68}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Empirical findings}{68}}
\newlabel{sec:evaluation}{{5.5}{68}}
\citation{zipf1949human}
\citation{breslau99}
\citation{lu10}
\newlabel{fig:zipfnoequiv}{{5.6(a)}{69}}
\newlabel{sub@fig:zipfnoequiv}{{(a)}{69}}
\newlabel{fig:reduction}{{5.6(b)}{69}}
\newlabel{sub@fig:reduction}{{(b)}{69}}
\newlabel{fig:reduction2}{{5.6(c)}{69}}
\newlabel{sub@fig:reduction2}{{(c)}{69}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces  \ref  {sub@fig:zipfnoequiv} Zipf's Law: subtree frequency plotted against subtree rank (in the frequency table). \ref  {sub@fig:reduction} Fraction of remaining unique ASTs after canonicalizing the $k$ most frequent ASTs with 1, 10 or 19 learned equivalence classes; \ref  {sub@fig:reduction2} Number of submissions covered if an instructor were to mark the 25 or 200 most frequent ASTs after canonicalization \relax }}{69}}
\newlabel{fig:zipfreduction}{{5.6}{69}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{69}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{69}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{69}}
\citation{huang13}
\newlabel{fig:bugsAll}{{5.7(a)}{71}}
\newlabel{sub@fig:bugsAll}{{(a)}{71}}
\newlabel{fig:bugIsolation}{{5.7(b)}{71}}
\newlabel{sub@fig:bugIsolation}{{(b)}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces  \ref  {sub@fig:bugsAll} F-score comparison of Codewebs based bug detection algorithm against baseline (5-nearest neighbors) for the 5000 most frequent ASTs for each assigned homework problem. Each circle corresponds to a single homework problem, with circle widths set to be proportional to the average \# of nodes per AST for that problem; \ref  {sub@fig:bugIsolation} Codewebs based bug detection F-scores on the $k$ most frequent ASTs, with and without canonicalization on the ``linear regression with gradient descent'' homework problem. \relax }}{71}}
\newlabel{fig:exp2}{{5.7}{71}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{71}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{71}}
\citation{sindhgatta06}
\citation{hoffmann07}
\citation{hummel08}
\citation{hartmann10}
\citation{paul94}
\citation{thummalapenta07}
\citation{kim10}
\citation{baxter98}
\citation{xu03}
\citation{rivers12}
\citation{rivers13}
\citation{hummel08}
\citation{lazzarini09}
\citation{piech12}
\citation{glassman13}
\citation{gross12}
\citation{gross13}
\citation{rivers12}
\citation{rivers13}
\citation{huang13}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Related work}{72}}
\newlabel{sec:relatedwork}{{5.6}{72}}
\citation{fast13}
\citation{kohlhase06}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Discussion}{73}}
\newlabel{sec:discussion}{{5.7}{73}}
\citation{sadler06}
\citation{kulkarni13}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Crowd Source Feedback}{75}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Peer-grading network: Each node is a learner with edges depicting who graded whom. Node size represents the number of graders for that student. The highlighted learner shown above graded five students (circular nodes) and was in turn graded by four students (square nodes). \relax }}{76}}
\newlabel{fig:gradingnetwork}{{6.1}{76}}
\citation{russell04}
\citation{kulkarni13}
\citation{kulkarni13}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Datasets}{77}}
\newlabel{sec:datasets}{{6.1}{77}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Data Sets\relax }}{78}}
\newlabel{tab:datasets}{{6.1}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Probabilistic models of peer grading in MOOCs}{78}}
\newlabel{fig:gradergrade_v_residual}{{6.2(a)}{79}}
\newlabel{sub@fig:gradergrade_v_residual}{{(a)}{79}}
\newlabel{fig:gradeegrade_v_residual}{{6.2(b)}{79}}
\newlabel{sub@fig:gradeegrade_v_residual}{{(b)}{79}}
\newlabel{fig:gradergradee}{{6.2(c)}{79}}
\newlabel{sub@fig:gradergradee}{{(c)}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \ref  {sub@fig:gradergrade_v_residual} The relationship between a grader's homework performance (her grade) and statistics (mean/standard deviation) of grading performance (residual from true grade). \ref  {sub@fig:gradeegrade_v_residual} The relationship between a gradee's homework performance against statistics of assessments for her submissions. \ref  {sub@fig:gradergradee} Visualization of all three variables simultaneously, where intensity reflects the mean residual $z$-score. Empty boxes mean that there is not enough data available to compute a reliable estimate. \relax }}{79}}
\newlabel{fig:graders}{{6.2}{79}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{79}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{79}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Models}{80}}
\@writefile{toc}{\contentsline {paragraph}{Model  $\unhbox \voidb@x \hbox {\bf  PG}_1$\tmspace  +\thinmuskip {.1667em}(Grader bias and reliability)}{80}}
\@writefile{toc}{\contentsline {paragraph}{Model  $\unhbox \voidb@x \hbox {\bf  PG}_2$\tmspace  +\thinmuskip {.1667em}(Temporal coherence)}{80}}
\@writefile{toc}{\contentsline {paragraph}{Model  $\unhbox \voidb@x \hbox {\bf  PG}_3$\tmspace  +\thinmuskip {.1667em}(Coupled grader score and reliability)}{81}}
\@writefile{toc}{\contentsline {paragraph}{Ethics and Incentives}{81}}
\citation{geman84}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Inference and evaluation.}{82}}
\citation{kulkarni13}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Comparison of models on the two HCI courses\relax }}{83}}
\newlabel{tab:results}{{6.2}{83}}
\@writefile{toc}{\contentsline {paragraph}{Evaluation}{83}}
\newlabel{fig:baseline}{{6.3(a)}{84}}
\newlabel{sub@fig:baseline}{{(a)}{84}}
\newlabel{fig:improved}{{6.3(b)}{84}}
\newlabel{sub@fig:improved}{{(b)}{84}}
\newlabel{fig:confidence}{{6.3(c)}{84}}
\newlabel{sub@fig:confidence}{{(c)}{84}}
\newlabel{fig:piechart}{{6.3(d)}{84}}
\newlabel{sub@fig:piechart}{{(d)}{84}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces  \ref  {sub@fig:baseline} Histogram of errors made using the baseline (median) scoring mechanism. \ref  {sub@fig:improved} Histogram of errors using  $\unhbox \voidb@x \hbox {\bf  PG}_3$\tmspace  +\thinmuskip {.1667em}. \ref  {sub@fig:confidence} A comparison of model confidence ($x$-axis) and actual success rate of predictions ($y$-axis), where being above the diagonal (dark bars) is better. \ref  {sub@fig:piechart} Number of submissions for which our model can declare ``confidence'' after $K$ rounds of grading. \relax }}{84}}
\newlabel{tab:predacc}{{6.3}{84}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{84}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{84}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{84}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Experimental results}{84}}
\newlabel{sec:experiments}{{6.3}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Accuracy of reweighted peer grading}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Fairness and efficiency in peer grading}{85}}
\newlabel{fig:time_v_residual}{{6.4(a)}{86}}
\newlabel{sub@fig:time_v_residual}{{(a)}{86}}
\newlabel{fig:roc}{{6.4(b)}{86}}
\newlabel{sub@fig:roc}{{(b)}{86}}
\newlabel{fig:comments}{{6.4(c)}{86}}
\newlabel{sub@fig:comments}{{(c)}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces  \ref  {sub@fig:time_v_residual} Grader consistency (measured using standard deviation of grading residual) as a function of time spent grading. \ref  {sub@fig:roc} ROC curve comparing performance (with linear SVM) at predicting future class participation given a student's grade, bias, reliability or all three. \ref  {sub@fig:comments} Commenting style (length of comment and sentiment polarity) as a function of grading residual. \relax }}{86}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{86}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{86}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Graders in the context of the MOOC}{86}}
\@writefile{toc}{\contentsline {paragraph}{Influential factors for grader ability}{86}}
\citation{nielsen11}
\@writefile{toc}{\contentsline {paragraph}{Grader ability and future performance}{87}}
\citation{baker01}
\citation{johnson96}
\citation{rogers10}
\citation{mislevy99}
\citation{batchelder88}
\citation{karabatsos03}
\citation{whitehill09}
\citation{johnson96}
\citation{rogers10}
\citation{goldin11}
\citation{ashley11}
\citation{goldin12}
\citation{goldin12}
\citation{bachrach12}
\citation{kamar12}
\citation{whitehill09}
\citation{charlin11}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Related work}{88}}
\newlabel{sec:relatedwork}{{6.4}{88}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Discussion and Future work}{89}}
\bibstyle{alpha}
\bibdata{references}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusions}{90}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{ahmadzadeh2005analysis}{AEH05}
\bibcite{aleven2002effective}{AK02}
\bibcite{barnes2008toward}{BS08}
\bibcite{cheang2003automated}{CKLO03}
\bibcite{fitzgerald2008debugging}{FLM{$^{+}$}08}
\bibcite{fleiss1981measurement}{FLP81}
\bibcite{huang2013syntactic}{HPNG13}
\bibcite{kulkarni2013peer}{KWL{$^{+}$}13}
\bibcite{nguyen2014codewebs}{NPHG14}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{91}}
\bibcite{pappano2012year}{Pap12}
\bibcite{piech2012modeling}{PSK{$^{+}$}12}
\bibcite{puterman2009markov}{Put09}
\bibcite{ritter2007cognitive}{RAKC07}
\bibcite{roll2014benefits}{RBAK14}
\bibcite{rivers2012canonicalizing}{RK12}
\bibcite{rivers2014automating}{RK14}
\bibcite{szczerba2000robust}{SGGT00}
\bibcite{singh2013automated}{SGSL13}
\bibcite{shapley1953stochastic}{Sha53}
\bibcite{weld2012personalized}{WAC{$^{+}$}12}
\bibcite{watson2012bluefix}{WLG12}
\bibcite{whitehill2009whose}{WWB{$^{+}$}09}
